{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reviews ( local ):\n",
    "\n",
    "    url = \"https://www.google.com/maps\"\n",
    "    \n",
    "    # Import the necessary libraries.\n",
    "    import os\n",
    "    import time\n",
    "    import pandas as pd\n",
    "    from selenium import webdriver\n",
    "    from selenium.webdriver.common.keys import Keys\n",
    "    from selenium.webdriver.common.by import By\n",
    "    from selenium.webdriver.support.ui import WebDriverWait\n",
    "    from selenium.webdriver.support import expected_conditions as EC\n",
    "    from selenium import webdriver\n",
    "    from selenium.webdriver.support.ui import Select\n",
    "    from selenium import webdriver\n",
    "    from bs4 import BeautifulSoup\n",
    "    from unidecode import unidecode\n",
    "\n",
    "    # Store the function's input as \"name\".\n",
    "    name = local\n",
    "\n",
    "    # Run the driver.\n",
    "    driver = webdriver.Chrome()\n",
    "    driver.get(url)\n",
    "\n",
    "    # Run through cookies page on Google.\n",
    "    button_xpath = \"//button[@jsname='b3VHJd']\"\n",
    "    button = WebDriverWait(driver, 10).until(EC.element_to_be_clickable((By.XPATH, button_xpath)))\n",
    "    button.click()\n",
    "\n",
    "    # Write the local's name in the search bar and run it.\n",
    "    searchbar = driver.find_element(By.ID, \"searchboxinput\")\n",
    "    searchbar.send_keys(name)\n",
    "    searchbar.send_keys(Keys.RETURN)\n",
    "\n",
    "    # Open the reviews section from that local's brochure.\n",
    "    button_xpath = \"//button[@jslog='145620; track:click;']\"\n",
    "    button = WebDriverWait(driver, 10).until(EC.element_to_be_clickable((By.XPATH, button_xpath)))\n",
    "    button.click()\n",
    "\n",
    "    # Execute BeautifulSoup to read what is in that same page.\n",
    "    html = driver.execute_script(\"return document.body.outerHTML;\")\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "    time.sleep(3)\n",
    "\n",
    "    # Set the number of reviews to scrape, 500 was a reasonable number as a higher one gave problems.\n",
    "    reviews_number = 100\n",
    "\n",
    "    # We scrape the number of reviews available in case they have less than 500, eitherwise we would be going into an infinte while loop later on.\n",
    "    reviews_available = int(soup.find_all(\"div\",{\"class\":\"fontBodySmall\"})[3].text.split(\" \")[0].replace(\",\", \"\"))\n",
    "    if reviews_available > reviews_number:\n",
    "        reviews_to_scrape = reviews_number\n",
    "    else:\n",
    "        reviews_to_scrape = reviews_available\n",
    "\n",
    "    # Open the sort button.\n",
    "    button_xpath = \"g88MCb\"\n",
    "    button =  WebDriverWait(driver, 10).until(EC.presence_of_all_elements_located((By.CLASS_NAME, button_xpath)))\n",
    "    button[2].click()\n",
    "\n",
    "    # Click the \"newest\" drop-down button to sort by date, ascending (newest first). We will be extractnig the last 500 reviews.\n",
    "    button_xpath = \"fxNQSd\"\n",
    "    newest = WebDriverWait(driver, 10).until(EC.presence_of_all_elements_located((By.CLASS_NAME, button_xpath)))\n",
    "    newest[1].click()\n",
    "\n",
    "    # Run the scrolling while loop until the number of reviews on the available scrolled page is higher than the set number of 500.\n",
    "    while len(soup.find_all('span', {\"class\":\"kvMYJc\"})) < reviews_to_scrape:\n",
    "        scroll = driver.find_element(By.XPATH, \"/html/body/div[3]/div[8]/div[9]/div/div/div[1]/div[2]/div/div[1]/div/div/div[2]\")\n",
    "        scroll.send_keys(Keys.PAGE_DOWN)\n",
    "        time.sleep(2)\n",
    "        html = driver.execute_script(\"return document.body.outerHTML;\")\n",
    "        soup = BeautifulSoup(html, \"html.parser\")\n",
    "        \n",
    "        print('\\r' + str(round((((len(soup.find_all('span', {\"class\":\"kvMYJc\"}))) / reviews_to_scrape) * 100), 2))+\"%\", end='', flush=True)\n",
    "\n",
    "    time.sleep(1)\n",
    "\n",
    "    # Once we have 500 reviews on our visible page, run through all of them and click the \"more\" button to access the full reviews.\n",
    "    button_xpath = \"w8nwRe\"\n",
    "    button = WebDriverWait(driver, 10).until(EC.presence_of_all_elements_located((By.CLASS_NAME, button_xpath)))\n",
    "\n",
    "    for i in button:\n",
    "        i.click()\n",
    "\n",
    "    time.sleep(3)\n",
    "\n",
    "    # BeautifulSoup to read all 500 reviews.\n",
    "    html = driver.execute_script(\"return document.body.outerHTML;\")\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "    # Create a ratings list with all the 500 ratings (from 1 to 5).\n",
    "    rating = soup.find_all('span', {\"class\":\"kvMYJc\"})\n",
    "    ratings = []\n",
    "\n",
    "    for i in rating:\n",
    "        ratings.append(i[\"aria-label\"][0])\n",
    "\n",
    "    # Create lists for the reviews, usernames, how long ago where the reviews made, and the link to that review to be able to answer the user's review.\n",
    "    reviews = []\n",
    "    usernames = []\n",
    "    times = []\n",
    "    linktoreviews = []\n",
    "\n",
    "    for i in range(len(ratings)):\n",
    "        try:\n",
    "            newratings = soup.find_all('div', {\"class\":\"jftiEf fontBodyMedium\"})[i].find('span', class_='wiI7pd').text\n",
    "            reviews.append(unidecode(newratings))\n",
    "\n",
    "        except AttributeError:\n",
    "            reviews.append(\"No Review\")\n",
    "        \n",
    "        username = soup.find_all('div', {\"class\":\"jftiEf fontBodyMedium\"})[i][\"aria-label\"]\n",
    "        usernames.append(unidecode(username))\n",
    "\n",
    "        time = soup.find_all('div', {\"class\":\"jftiEf fontBodyMedium\"})[i].find('span', class_='rsqaWe').text\n",
    "        times.append(time)\n",
    "\n",
    "        linktoreview = soup.find_all('div', {\"class\":\"jftiEf fontBodyMedium\"})[i].find('button', class_='al6Kxe')[\"data-href\"]\n",
    "        linktoreviews.append(linktoreview)\n",
    "    \n",
    "    # System to transform the time data into days (3 months ago = 90, 2 weeks ago = 14, and so on).\n",
    "    times2 = []\n",
    "    days = []\n",
    "\n",
    "    for i in times:\n",
    "        times2.append(i.split(\" \")[:2])\n",
    "\n",
    "    index = {\"minute\": 1440, \"hour\": 24, \"day\": 1, \"week\": 7, \"month\": 30, \"year\": 365}\n",
    "\n",
    "    for j in times2:\n",
    "        if j[0] == (\"a\" or \"an\"):\n",
    "            for i in index.keys():\n",
    "                if i in j[1]:\n",
    "                    if i == \"hour\":\n",
    "                        days.append(1 / index[i])\n",
    "                    elif i == \"minute\":\n",
    "                        days.append(1 / index[i])\n",
    "                    else:\n",
    "                        days.append(1 * index[i])\n",
    "        else: \n",
    "            for i in index.keys():\n",
    "                if i in j[1]:\n",
    "                    if i == \"hour\":\n",
    "                        days.append(int(j[0]) / index[i])\n",
    "                    elif i == \"minute\":\n",
    "                        days.append(int(j[0]) / index[i])\n",
    "                    else:\n",
    "                        days.append(int(j[0]) * index[i])\n",
    "\n",
    "    # Create the dictionary where we will store all the extracted data.  \n",
    "    table = {}\n",
    "    table[\"restaurant\"] = name\n",
    "    table[\"username\"] = usernames\n",
    "    table[\"review\"] = reviews\n",
    "    table[\"rating\"] = ratings\n",
    "    table[\"daysago\"] = days\n",
    "    table[\"linktoreview\"] = linktoreviews\n",
    "\n",
    "    # Convert dictionary to pandas dataframe.\n",
    "    restaurant = pd.DataFrame(table)\n",
    "\n",
    "    # Export as csv file with the name of the file to be the inputted local's name.\n",
    "    restaurant.to_csv(f\"{name}.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def information ( local ):\n",
    "    import os\n",
    "    import time\n",
    "    import pandas as pd\n",
    "\n",
    "    from selenium import webdriver\n",
    "    from selenium.webdriver.common.keys import Keys\n",
    "    from selenium.webdriver.common.by import By\n",
    "    from selenium.webdriver.support.ui import WebDriverWait\n",
    "    from selenium.webdriver.support import expected_conditions as EC\n",
    "    from selenium import webdriver\n",
    "    from selenium.webdriver.support.ui import Select\n",
    "    from selenium import webdriver\n",
    "\n",
    "    from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "    from bs4 import BeautifulSoup\n",
    "    from unidecode import unidecode\n",
    "    import nltk\n",
    "\n",
    "    url = \"https://www.google.com/maps\"\n",
    "\n",
    "    name = local\n",
    "\n",
    "    driver = webdriver.Chrome()\n",
    "    driver.get(url)\n",
    "\n",
    "    button_xpath = \"//button[@jsname='b3VHJd']\"\n",
    "    button = WebDriverWait(driver, 10).until(EC.element_to_be_clickable((By.XPATH, button_xpath)))\n",
    "    button.click()\n",
    "\n",
    "    searchbar = driver.find_element(By.ID, \"searchboxinput\")\n",
    "    searchbar.send_keys(name)\n",
    "    searchbar.send_keys(Keys.RETURN)\n",
    "\n",
    "    time.sleep(5)\n",
    "\n",
    "    html = driver.execute_script(\"return document.body.outerHTML;\")\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "    company = []\n",
    "    company.append(soup.find_all(\"h1\", {\"class\":\"DUwDvf lfPIob\"})[0].text)\n",
    "\n",
    "    description = []\n",
    "    description.append(soup.find_all(\"button\", {\"class\":\"DkEaL\"})[0].text)\n",
    "\n",
    "    address = []\n",
    "    address.append(soup.find_all(\"div\", {\"class\":\"Io6YTe fontBodyMedium kR99db\"})[0].text)\n",
    "\n",
    "    information = {}\n",
    "    information[\"company\"] = company\n",
    "    information[\"description\"] = description\n",
    "    information[\"address\"] = address\n",
    "\n",
    "    informations = pd.DataFrame(information)\n",
    "    # informations.to_csv(\"locals.csv\")\n",
    "\n",
    "    locals = pd.read_csv(\"locals.csv\")\n",
    "    locals = locals.iloc[:, 1:]\n",
    "\n",
    "    table = pd.concat([locals, informations])\n",
    "\n",
    "    # table = table.iloc[:, 1:]\n",
    "    table = table.drop_duplicates().reset_index()\n",
    "    table.drop(columns=['index'], inplace=True)\n",
    "    table.to_csv(\"locals.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<selenium.webdriver.remote.webelement.WebElement (session=\"0155e7335c0ec52f503f7312312f69f0\", element=\"745F47A6556A502864F2678A62219337_element_126\")>\n",
      "10.0%<selenium.webdriver.remote.webelement.WebElement (session=\"0155e7335c0ec52f503f7312312f69f0\", element=\"745F47A6556A502864F2678A62219337_element_126\")>\n",
      "10.0%<selenium.webdriver.remote.webelement.WebElement (session=\"0155e7335c0ec52f503f7312312f69f0\", element=\"745F47A6556A502864F2678A62219337_element_126\")>\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_10848\\1092513556.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mlocal\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mreviews\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlocal\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0minformation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlocal\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_10848\\179924196.py\u001b[0m in \u001b[0;36mreviews\u001b[1;34m(local)\u001b[0m\n\u001b[0;32m     70\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mscroll\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     71\u001b[0m         \u001b[0mscroll\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_keys\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mKeys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPAGE_DOWN\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 72\u001b[1;33m         \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     73\u001b[0m         \u001b[0mhtml\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdriver\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexecute_script\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"return document.body.outerHTML;\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     74\u001b[0m         \u001b[0msoup\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mBeautifulSoup\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhtml\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"html.parser\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "local = input()\n",
    "reviews(local)\n",
    "information(local)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"locals.csv\", index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"locals.csv\", index_col=0)\n",
    "df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
